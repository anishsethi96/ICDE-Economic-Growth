# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wHkg-s0iqcbKZW7J5ti-nhtYnEZVYrqw
"""

import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.decomposition import PCA 
from sklearn.cluster import AgglomerativeClustering 
from sklearn.preprocessing import StandardScaler, normalize 
from sklearn.metrics import silhouette_score 
import scipy.cluster.hierarchy as shc 
import math
from numpy import linalg

# Changing the working location to the location of the file   
X = pd.read_csv('Dataset.csv') 
  
# Dropping the CUST_ID column from the data 
X = X.drop('Country or Area', axis = 1)
X = X.drop('Year', axis = 1)
  
# Handling the missing values 
X.fillna(method ='ffill', inplace = True) 
X

n,d=X.shape

mean = np.sum(X) / n
df_center = X-mean

inner_cov = np.dot((df_center).T, (df_center))/n
print("Inner Covariance: ")

w, v = np.linalg.eigh(inner_cov)
w = w[::-1]
v = v[::-1]
print(w)

total_var = np.sum(w)
print(total_var)

alpha = 0.90
for i in range(0,d):
  partial_var = 0
  for r in range(0,i+1):
    partial_var = partial_var + w[r]
    if (partial_var/total_var > float(alpha)):
      break;

print(f"{r+1} dimensions are required to capture Î± = {alpha*100} % of the total variance")

# Scaling the data so that all the features become comparable 
scaler = StandardScaler() 
X_scaled = scaler.fit_transform(X) 
  
# Normalizing the data so that the data approximately  
# follows a Gaussian distribution 
X_normalized = normalize(X_scaled) 
  
# Converting the numpy array into a pandas DataFrame 
X_normalized = pd.DataFrame(X_normalized)

X_normalized

pca = PCA(n_components = 2) 
X_principal = pca.fit_transform(X_normalized) 
X_principal = pd.DataFrame(X_principal) 
X_principal.columns = ['P1', 'P2'] 
print(X_principal)

plt.figure(figsize =(8, 8)) 
plt.title('Visualising the data') 
Dendrogram = shc.dendrogram((shc.linkage(X_principal, method ='centroid')))

import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import make_blobs
from sklearn.cluster import MeanShift, estimate_bandwidth

X = X_principal

ac2 = AgglomerativeClustering(n_clusters = 5) 
  
# Visualizing the clustering 
plt.figure(figsize =(6, 6)) 
plt.scatter(X_principal['P1'], X_principal['P2'],  
           c = ac2.fit_predict(X_principal), cmap ='rainbow') 
plt.show() 

print(ac2.fit_predict(X_principal))

# Estimate bandwith
bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)

# Fit Mean Shift with Scikit
meanshift = MeanShift(bandwidth=bandwidth)
meanshift.fit(X)
labels = meanshift.labels_
labels_unique = np.unique(labels)
n_clusters_ = len(labels_unique)

# Predict the cluster for all the samples
P = meanshift.predict(X)
plt.figure(figsize =(6, 6)) 
plt.scatter(X_principal['P1'], X_principal['P2'],  
           c = meanshift.predict(X), cmap ='rainbow') 
plt.show() 

print(meanshift.predict(X))

# Import required libraries and modules 
import matplotlib.pyplot as plt 
from sklearn.datasets.samples_generator import make_blobs 
from sklearn.cluster import Birch 
  
# Generating 600 samples using make_blobs 
dataset, clusters = make_blobs(n_samples = 600, centers = 8, cluster_std = 0.75, random_state = 0) 
  
# Creating the BIRCH clustering model 
model = Birch(branching_factor = 50, n_clusters = None, threshold = 1.5) 
  
# Fit the data (Training) 
model.fit(X_principal) 
  
# Predict the same data 
pred = model.predict(X_principal) 

dataset
X_principal
pred